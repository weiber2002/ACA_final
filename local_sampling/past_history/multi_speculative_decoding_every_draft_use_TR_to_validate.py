import torch
from torch.nn import Module
from utils.logits_processor import LogitsProcessor, GreedyProcessor
from transformers.cache_utils import DynamicCache
from utils.caching import prune_cache
import utils.printing as printing
from typing import List, Tuple, Dict, Any
import time


def max_fn(x: torch.Tensor) -> torch.Tensor:
    """
    Max function.
        x: input tensor.
    Returns:
        tensor norm(max(0, x)).
    """
    x_max = torch.where(x > 0, x, torch.zeros_like(x))
    x_max_sum = torch.sum(x_max, dim=-1, keepdim=True)
    return x_max / x_max_sum


@torch.no_grad()
def multi_draft_speculative_generate(
    inputs: List[int],
    drafters: List[Tuple[str, Module]],
    target: Module,
    tokenizer = None,
    gamma: int = 5,
    logits_processor: LogitsProcessor = GreedyProcessor(),
    max_gen_len: int = 40,
    eos_tokens_id: int | List[int] = 1,
    pad_token_id: int = 0,
    use_cache: bool = True,
    skip_sample_adjustment: bool = False,
    first_target: bool = True,
    debug: bool = False,
) -> Tuple[List[int], Dict[str, float]]:
    """
    Generate text sequence using multi-drafter speculative decoding algorithm.
    Implementation uses multiple draft models to generate at each step.
    
    Args:
        inputs (List[int]): input sequence of batch size 1.
        drafters (List[Tuple[str, Module]]): list of (name, drafter_model) tuples.
        target (Module): target model.
        tokenizer: tokenizer (used for debugging).
        gamma (int): number of drafts generated by each drafter at each step.
        logits_processor (LogitsProcessor): logits processor for sampling.
        max_gen_len (int): maximum length of the generated sequence.
        eos_tokens_id (int or List[int]): end token id (could be multiple).
        pad_token_id (int): pad token id.
        use_cache (bool): whether to use cache.
        skip_sample_adjustment (bool): whether to skip the sample adjustment step when some drafts are discarded.
        first_target (bool): whether to run the target model before the speculative algorithm.
        debug (bool): debug mode.
    
    Returns:
        List[int]: generated sequence.
        Dict[str, float]: statistics including acceptance rate and drafter usage counts.
    """
    
    # Initialize caches for all drafters and target
    drafter_caches = {name: None for name, _ in drafters}
    target_cache = None

    list_tokens_id = eos_tokens_id if isinstance(eos_tokens_id, list) else [eos_tokens_id]
    stop_tokens = torch.tensor(list_tokens_id, dtype=torch.long, device=target.device).unsqueeze(1)
    
    # Statistics tracking
    drafts_accepted = 0.0
    drafts_speculated = 0.0
    drafter_usage_counts = {name: 0 for name, _ in drafters}
    
    vocabulary_size = target.config.vocab_size    
        
    # prepare input tensor
    prompt_len = len(inputs)
    max_seq_length = target.config.max_position_embeddings if hasattr(target.config, 'max_position_embeddings') else (target.config.max_context_length if hasattr(target.config, 'max_context_length') else 1024)
    total_len = min(max_seq_length, prompt_len + max_gen_len)
    input_ids = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=target.device)
    input_ids[0, :prompt_len] = torch.tensor(inputs, dtype=torch.long, device=target.device)
    
    current_position = prompt_len
    
    if first_target:
        # Run the target model before the speculative algorithm
        Mp = target(
            input_ids=input_ids[..., :current_position],
            past_key_values=target_cache,
            use_cache=use_cache,
        )
        target_cache = Mp.past_key_values
        p_p = logits_processor(Mp.logits[..., -1, :])
        t = logits_processor.sample(p_p)
        input_ids[0, current_position] = t
        current_position += 1
        
        if torch.isin(t, stop_tokens):
            if debug:
                printing.end_token_found(0)
            return input_ids[0, prompt_len:current_position].tolist(), {
                "acc_rate": 0,
                "drafter_usage": drafter_usage_counts
            }
        
        if debug:
            printing.initial_step(t, tokenizer)
    
    while current_position < total_len:
        corrected_gamma = min(gamma, total_len - current_position - 1)
        
        # Dictionary to hold proposals from all drafters
        all_proposals = {}
        
        # Generate proposals from each drafter
        for drafter_name, drafter in drafters:
            # Create temporary input tensor for this drafter
            drafter_input_ids = input_ids.clone().to(drafter.device)
            drafter_q = torch.zeros((1, corrected_gamma, vocabulary_size), device=drafter.device)
            
            # Generate gamma drafts from this drafter
            for k in range(corrected_gamma):
                Mq = drafter(
                    input_ids=drafter_input_ids[..., :current_position + k],
                    past_key_values=drafter_caches[drafter_name],
                    use_cache=use_cache,
                )
                drafter_caches[drafter_name] = Mq.past_key_values
                
                draft_logits = Mq.logits[..., -1, :]
                draft_probs = logits_processor(draft_logits)
                drafter_q[0, k] = draft_probs
                xi = logits_processor.sample(draft_probs)
                drafter_input_ids[0, current_position + k] = xi
            
            # Store proposals from this drafter
            all_proposals[drafter_name] = {
                "input_ids": drafter_input_ids.to(target.device),
                "q": drafter_q.to(target.device)
            }
        
        # Evaluate all proposals and select the best one
        best_drafter = None
        best_acceptance = -1
        best_fractions = None
        best_p = None
        best_q = None
        best_input_ids = None
        
        for drafter_name, proposal in all_proposals.items():
            # Run target model on these drafts
            prop_input_ids = proposal["input_ids"]
            prop_q = proposal["q"]
            
            Mp = target(
                input_ids=prop_input_ids[..., :current_position + corrected_gamma],
                past_key_values=target_cache,
                use_cache=False,  # Don't save cache yet, just evaluating
            )
            
            draft_logits = Mp.logits[..., current_position - 1:current_position + corrected_gamma - 1, :]
            p = logits_processor(draft_logits)
            
            # Compute acceptance using rejection sampling
            fractions = p / prop_q
            r = torch.rand(corrected_gamma, device=target.device)
            acceptance = corrected_gamma
            for i in range(corrected_gamma):
                if r[i] > fractions[0, i, prop_input_ids[0, current_position + i]]:
                    acceptance = i
                    break
            
            # Check if this drafter performed better
            if acceptance > best_acceptance:
                best_drafter = drafter_name
                best_acceptance = acceptance
                best_fractions = fractions
                best_p = p
                best_q = prop_q
                best_input_ids = prop_input_ids.clone()
        
        # Now use the best drafter's proposals
        input_ids = best_input_ids
        drafter_usage_counts[best_drafter] += 1
        
        # Rerun target model with best drafter's proposals to update cache
        Mp = target(
            input_ids=input_ids[..., :current_position + corrected_gamma],
            past_key_values=target_cache,
            use_cache=use_cache,
        )
        target_cache = Mp.past_key_values
        
        n = best_acceptance
        drafts_speculated += corrected_gamma
        drafts_accepted += n
        
        # Check if the end token is in the drafts
        stop_locations = torch.nonzero(torch.eq(input_ids[..., current_position:current_position + n], stop_tokens))
        if stop_locations.shape[0] > 0:
            stop_location = stop_locations[0, 1].item()
            if debug:
                printing.end_token_found(stop_location)
            return input_ids[0, prompt_len:current_position + stop_location + 1].tolist(), {
                "acc_rate": drafts_accepted / drafts_speculated,
                "drafter_usage": drafter_usage_counts
            }
        
        # Adjust the distribution from Mp
        if n == corrected_gamma:
            p_p = Mp.logits[..., current_position + corrected_gamma - 1, :]
            p_p = logits_processor(p_p)
        else:
            # Prune the cache
            if use_cache:
                for d_name in drafter_caches:
                    drafter_caches[d_name] = prune_cache(drafter_caches[d_name], corrected_gamma - n)
                target_cache = prune_cache(target_cache, corrected_gamma - n + 1)
            
            if not skip_sample_adjustment:
                p_p = max_fn(best_p[..., n, :] - best_q[0, n, :])
            else:
                p_p = best_p[..., n, :]
        
        x = logits_processor.sample(p_p)
        
        if debug:
            generated = input_ids.clone().detach()
        
        input_ids[0, current_position + n:current_position + corrected_gamma] = pad_token_id
        input_ids[0, current_position + n] = x
        
        if debug:
            printing.speculative_step(tokenizer, generated, input_ids, n, prompt_len, current_position, corrected_gamma)
            print(f"Best drafter: {best_drafter}, Accepted: {n} tokens")
        
        current_position += n + 1
        
        if torch.isin(x, stop_tokens):
            if debug:
                printing.end_token_found(n)
            return input_ids[0, prompt_len:current_position].tolist(), {
                "acc_rate": drafts_accepted / drafts_speculated,
                "drafter_usage": drafter_usage_counts
            }
    
    return input_ids[0, prompt_len:].tolist(), {
        "acc_rate": drafts_accepted / drafts_speculated,
        "drafter_usage": drafter_usage_counts
    }