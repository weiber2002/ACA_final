import torch
from torch.nn import Module
from utils.logits_processor import LogitsProcessor, GreedyProcessor
from transformers.cache_utils import DynamicCache
from utils.caching import prune_cache
import utils.printing as printing
from typing import List, Tuple


def max_fn(x: torch.Tensor) -> torch.Tensor:
    """
    Max function.
        x: input tensor.
    Returns:
        tensor norm(max(0, x)).
    """
    x_max = torch.where(x > 0, x, torch.zeros_like(x))
    x_max_sum = torch.sum(x_max, dim=-1, keepdim=True)
    return x_max / x_max_sum


@torch.no_grad()
def cache_speculative_generate(
    inputs: List[int],
    drafter: Module,
    target: Module,
    tokenizer = None,
    gamma: int = 5,
    logits_processor: LogitsProcessor = GreedyProcessor(),
    max_gen_len: int = 40,
    eos_tokens_id: int | List[int] = 1,
    pad_token_id: int = 0,
    use_cache: bool = False,
    skip_sample_adjustment: bool = False,
    first_target: bool = True,
    debug: bool = False,
) -> Tuple[List[int], float]:
    """
    Generate text sequence using the speculative decoding algorithm.
    Implementation of Speculative Decoding. (https://arxiv.org/pdf/2211.17192.pdf)
    
    Args:
        inputs (List[int]): input sequence of batch size 1.
        drafter (Module): drafter model.
        target (Module): target model.
        tokenizer: tokenizer (used for debugging).
        gamma (int): number of drafts generated by the drafter at each step.
        logits_processor (LogitsProcessor): logits processor for sampling.
        max_gen_len (int): maximum length of the generated sequence.
        eos_tokens_id (int or List[int]): end token id (could be multiple).
        pad_token_id (int): pad token id.
        use_cache (bool): whether to use cache.
        skip_sample_adjustment (bool): whether to skip the sample adjustment step when some drafts are discarded.
        first_target (bool): whether to run the target model before the speculative algorithm.
        debug (bool): debug mode.
    
    Returns:
        List[int]: generated sequence.
        float: acceptance rate (number of accepted drafts divided by the number of total drafts).
        
    Note: This generation methods only works for decoder-only models.
    Note bis: The drafter and target models should output the same logits shape.
    Note ter: NgramModels are currently not supported.
    """
    if use_cache == True:
        drafter_cache, target_cache = None, None
        list_tokens_id = eos_tokens_id if isinstance(eos_tokens_id, list) else [eos_tokens_id]
        stop_tokens = torch.tensor(list_tokens_id, dtype=torch.long, device=target.device).unsqueeze(1)
        
        drafts_accepted, drafts_speculated = .0, .0
        
        vocabulary_size = target.config.vocab_size

        prompt_len = len(inputs)
        max_seq_length = target.config.max_position_embeddings if hasattr(target.config, 'max_position_embeddings') else (target.config.max_context_length if hasattr(target.config, 'max_context_length') else 1024)
        total_len = min(max_seq_length, prompt_len + max_gen_len)
        input_ids_target = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=target.device)
        input_ids_drafter = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=drafter.device)
        input_ids_target[0, :prompt_len] = torch.tensor(inputs, dtype=torch.long, device=target.device)
        input_ids_drafter[0, :prompt_len] = torch.tensor(inputs, dtype=torch.long, device=drafter.device)

        position_ids_buffer_target = torch.arange(max_seq_length).unsqueeze(0).to( device=target.device)
        position_ids_buffer_drafter = torch.arange(max_seq_length).unsqueeze(0).to(device=drafter.device)
        current_position_target = prompt_len 
        current_position_drafter = prompt_len 

        #print("prompt_len  is " , prompt_len )
        if first_target:

            Mp = target(
                input_ids=input_ids_target[..., :current_position_target],
                past_key_values=target_cache,
                use_cache=use_cache,
                position_ids = position_ids_buffer_target[..., :current_position_target]
            )

            
            target_cache = Mp.past_key_values
            p_p = logits_processor(Mp.logits[..., -1, :])
            t = logits_processor.sample(p_p)
            input_ids_target [0, current_position_target] = t

            next_token_send_to_device = t.to(drafter.device)
            input_ids_drafter[0,current_position_drafter] = next_token_send_to_device

            
            Mq = drafter(
                input_ids =input_ids_drafter[..., :current_position_drafter], #don't need to summarize the new token from 
                past_key_values=drafter_cache,

                use_cache=use_cache,
                position_ids = position_ids_buffer_drafter[..., :current_position_drafter]
            )
            drafter_cache = Mq.past_key_values
            current_position_target += 1
            current_position_drafter += 1

            if torch.isin(t, stop_tokens):
                return input_ids_target[0, prompt_len:current_position_target].tolist(), 0
        #print("AFter first generation current_position_target is, "  ,current_position_target)
        #print("AFter first generation current_position_drafter is, " , current_position_drafter)

        #print("Before generation , input_ids_drafter[0 , : current_position_drafter] is :  " , input_ids_drafter[0 , : current_position_drafter] )

        all_accept_last_iteration = False
        q = torch.zeros((1, gamma, vocabulary_size), device=drafter.device)
        while (current_position_drafter+2) < total_len:

            # start_event_drafter = torch.cuda.Event(enable_timing=True)
            # end_event_drafter = torch.cuda.Event(enable_timing=True)
            # start_event_drafter.record()
            corrected_gamma = min(gamma, total_len - current_position_drafter -1 ) # current_position_drafter is now the new token place but we also need to reserve a place for verifier genertate
            
            if all_accept_last_iteration == True:
                Mq = drafter(
                    input_ids=input_ids_drafter[:, current_position_drafter + -2].unsqueeze(0),  # current_position_drafter is now the new token place, so we use the token before it as input
                    past_key_values=drafter_cache,
                    use_cache=use_cache,
                    position_ids = position_ids_buffer_drafter[ : , current_position_drafter -2].unsqueeze(0)

                )
                drafter_cache = Mq.past_key_values
            for k in range(corrected_gamma):

                #print("shape of position_ids_buffer_drafter[ : , current_position_drafter + k-1].unsqueeze(0) is " , position_ids_buffer_drafter[ : , current_position_drafter + k-1].unsqueeze(0).shape)
                #print("drafter_cache[0].key_cache.shape shape is " , drafter_cache[0][0].shape )
                #print("input_ids_drafter[:, current_position_drafter + k-1].unsqueeze(0) shape is " ,input_ids_drafter[:, current_position_drafter + k-1].unsqueeze(0).shape )
                Mq = drafter(
                    input_ids=input_ids_drafter[:, current_position_drafter + k-1].unsqueeze(0),  # current_position_drafter is now the new token place, so we use the token before it as input
                    past_key_values=drafter_cache,
                    use_cache=use_cache,
                    position_ids = position_ids_buffer_drafter[ : , current_position_drafter + k-1].unsqueeze(0)

                )
                drafter_cache = Mq.past_key_values
                draft_logits = Mq.logits
                '''
                torch.cuda.synchronize()
                print("in loop drafter_cache[0].key_cache.shape  is " , drafter_cache[0][0].shape, flush=True)
                torch.cuda.synchronize()
                '''
                #print("draft_logits.shape  is " , draft_logits.shape)
                draft_probs = logits_processor(draft_logits)
                q[0, k] = draft_probs
                xi = logits_processor.sample(draft_probs)
                input_ids_drafter[0, current_position_drafter + k] = xi # current_position_drafter is now the new token place, so we directly insert to it
            #print("input_ids_drafter[0 , : current_position_drafter+corrected_gamma] is :  " , input_ids_drafter[0 , : current_position_drafter+corrected_gamma] )
            # torch.cuda.synchronize()
            # end_event_drafter.record()
            # torch.cuda.synchronize()

# Compute elapsed time in milliseconds
            # elapsed_time_ms_drafter = start_event_drafter .elapsed_time(end_event_drafter )
            # print(f"drafter calculate Elapsed time: {elapsed_time_ms_drafter :.3f} ms")

            # start_event_drafter_to_target = torch.cuda.Event(enable_timing=True)
            # end_event_drafter_to_target = torch.cuda.Event(enable_timing=True)
            # start_event_drafter_to_target.record()

            drafts_speculated += corrected_gamma 
            target_q_buffer = q[0, :corrected_gamma , :].to(target.device)   #only need to send q to target to verify
            target_ids_buffer = input_ids_drafter[ 0 ,   current_position_drafter -1 : current_position_drafter+ corrected_gamma].to(target.device) #we will send old x to target since last iteration we don't input x at target size, easy for programming
            #print("target_ids_buffer.unsqueeze(0) shape is : " , target_ids_buffer.unsqueeze(0).shape)
            #print("target_cache shape is " , target_cache[0][0].shape )
            #print("position_ids_buffer_target [   ... ,  current_position_target -1 : current_position_target  +corrected_gamma    ] shape is ", position_ids_buffer_target [   ... , current_position_target -1 : current_position_target  +corrected_gamma   ].shape)
            #print("position_ids_buffer_target [   ... ,  current_position_target -1 : current_position_target  +corrected_gamma    ] is " , position_ids_buffer_target [   ... , current_position_target -1 : current_position_target  +corrected_gamma    ])
            
            # torch.cuda.synchronize()
            # end_event_drafter_to_target.record()
            # torch.cuda.synchronize()

            # elapsed_time_ms_drafter_to_target = start_event_drafter_to_target .elapsed_time(end_event_drafter_to_target )
            # print(f"drafter to target  Elapsed time: {elapsed_time_ms_drafter_to_target :.3f} ms")
            

            # start_event_target_calculate = torch.cuda.Event(enable_timing=True)
            # end_event_target_calculate = torch.cuda.Event(enable_timing=True)
            # start_event_target_calculate.record()

            Mp = target(
                input_ids=target_ids_buffer.unsqueeze(0), #[x ,  corrected_gamma this iteration ]
                past_key_values=target_cache,
                use_cache=use_cache,
                position_ids = position_ids_buffer_target [   ... , current_position_target -1 : current_position_target  +corrected_gamma    ] #  (x at last iteration ,corrected_gamma this iteration ) current_position_target is the new token postion after x, so x is start at current_position_target -1
            )
            target_cache = Mp.past_key_values
            target_logits = Mp.logits #         [1, 1+corrected_gamma  , vocab_size]
            p = logits_processor(target_logits) # [1, 1+gamma, vocab_size]


            # torch.cuda.synchronize()
            # end_event_target_calculate.record()
            # torch.cuda.synchronize()

            # elapsed_time_ms_target_calculate = start_event_target_calculate .elapsed_time(end_event_target_calculate )
            # print(f"target calculate Elapsed time: {elapsed_time_ms_target_calculate :.3f} ms")


            # start_event_target_compare = torch.cuda.Event(enable_timing=True)
            # end_event_target_compare = torch.cuda.Event(enable_timing=True)
            # start_event_target_compare.record()

            # compute the last accepted draft position (rejection sampling)
            r = torch.rand(corrected_gamma, device=target.device)
            #print("size of p is" , p.shape)
            # print("size of target_q_buffer is" , target_q_buffer.shape)
            #fractions = p[0 ,  : corrected_gamma ] / target_q_buffer
            # print("size of fractions is" , fractions.shape)
            n = corrected_gamma #if all acept
            for i in range(corrected_gamma): #we calulate first corrected_gamma [x ~ corrected_gamma-1 ] 
                p_select = p[0 , i, target_ids_buffer[i+1]]
                q_select = target_q_buffer[i, target_ids_buffer[i+1]]

                fraction = p_select / q_select
                # if r[i] > fractions[ i, target_ids_buffer[i+1]]: # however the look up table need to see [ i+1 ] (the result)
                #     n = i #change accept num if this token is not veriied
                #     break

                if r[i] > fraction:
                    n = i #change accept num if this token is not veriied
                    break

            
            drafts_accepted += n

            # check if the end token is in the drafts
            stop_locations = torch.nonzero(torch.eq(target_ids_buffer[..., : n], stop_tokens))
            if stop_locations.shape[0] > 0:
                stop_location = stop_locations[0, 1].item()
                if debug:
                    printing.end_token_found(stop_location)
                return input_ids_drafter[0, prompt_len:current_position_drafter + stop_location +1].tolist(), drafts_accepted / drafts_speculated

            # adjust the distribution from Mp

            p_p = Mp.logits[..., - 1, : ]

            # torch.cuda.synchronize()
            # end_event_target_compare.record()
            # torch.cuda.synchronize()

            # elapsed_time_ms_target_compare = start_event_target_compare .elapsed_time(end_event_target_compare )
            # print(f"target compare  Elapsed time: {elapsed_time_ms_target_compare :.3f} ms")

            # start_event_prune_cache = torch.cuda.Event(enable_timing=True)
            # end_event_prune_cache = torch.cuda.Event(enable_timing=True)
            # start_event_prune_cache.record()

            '''
            torch.cuda.synchronize()
            print("before prune cache drafter cahce size is  " , drafter_cache[0][0].shape, flush=True)
            torch.cuda.synchronize()
            print("before prune cache  target cahce size is  " , target_cache[0][0].shape, flush=True)
            torch.cuda.synchronize()

            torch.cuda.synchronize()
            print("verify token num n is " , n , flush=True)
            torch.cuda.synchronize()
            print("correct gamma is " , corrected_gamma , flush=True)
            '''
            if n == corrected_gamma: #all accept
                all_accept_last_iteration = True
                
                # p_p = Mp.logits[..., - 1, :] #sample the last one item
                # p_p = logits_processor(p_p)
            else:
                all_accept_last_iteration = False
                drafter_cache = prune_cache(drafter_cache, max(0, corrected_gamma - n -1)) #the n token is reject, means we need to prune the cache in drafter after this token
                target_cache = prune_cache(target_cache, corrected_gamma - n ) #the n token is reject, means we need to prune the cache in drafter after this token
                
                if not skip_sample_adjustment:
                    p_p = max_fn(p[..., n, :] - q[0, n, :])
                else:
                    p_p = p[..., n, :]
            x = logits_processor.sample(p_p)
            
            if debug:
                generated = input_ids.clone().detach()


            # torch.cuda.synchronize()
            # end_event_prune_cache.record()
            # torch.cuda.synchronize()

            # elapsed_time_ms_prune_cache = start_event_prune_cache .elapsed_time(end_event_prune_cache )
            # print(f"Pruene Caache  Elapsed time: {elapsed_time_ms_prune_cache:.3f} ms")
   

            # start_event_target_to_drafter = torch.cuda.Event(enable_timing=True)
            # end_event_target_to_drafter = torch.cuda.Event(enable_timing=True)
            # start_event_target_to_drafter.record()

            input_ids_target [0, current_position_target : current_position_target+n ] = target_ids_buffer[...,  1 :n+1 ] #skip the x
            input_ids_target[0, current_position_target + n] = x


            #print("verify token num n is " , n )
            #print("correct gamma is " , corrected_gamma )
            #print("at target now  input_ids_target[0, : current_position_target + n +1] is " , input_ids_target[0, : current_position_target + n +1])
            drafter_ids_buffer  = x.to(drafter.device)
            input_ids_drafter[ 0 , current_position_drafter + n ] =  drafter_ids_buffer
            


            current_position_drafter +=  ( n + 1) #drafter will append n (verified) + 1 (from target generate)
            current_position_target +=  ( n + 1) #drafter will append n (verified) + 1 (from target generate)


            # torch.cuda.synchronize()
            # end_event_target_to_drafter.record()
            # torch.cuda.synchronize()

            # elapsed_time_ms_target_to_drafter = start_event_target_to_drafter.elapsed_time(end_event_target_to_drafter )
            # print(f"Target to Drafter  Elapsed time: {elapsed_time_ms_target_to_drafter:.3f} ms")

            '''
            torch.cuda.synchronize()
            print("input_ids_drafter [   ... ,  : current_position_target    ] is " , input_ids_drafter [   ... , : current_position_drafter      ], flush=True)
            torch.cuda.synchronize()
            print("input_ids_target [   ... ,  : current_position_target    ] is " , input_ids_target[   ... , : current_position_target      ], flush=True)
            torch.cuda.synchronize()
            print("after prune cache drafter cahce size is  " , drafter_cache[0][0].shape, flush=True)
            torch.cuda.synchronize()
            print("after prune cache  target cahce size is  " , target_cache[0][0].shape, flush=True)
            torch.cuda.synchronize()
            print("current_position_drafter is ", current_position_drafter, flush=True)
            torch.cuda.synchronize()
            '''
            if debug:
                printing.speculative_step(tokenizer, generated, input_ids, n, prompt_len, current_position, corrected_gamma)
                
            
            if torch.isin(x, stop_tokens):
                if debug:
                    printing.end_token_found(n)
                return input_ids_drafter[0, prompt_len:current_position_drafter].tolist(), drafts_accepted / drafts_speculated


        return input_ids_drafter[0, prompt_len:].tolist(), drafts_accepted / drafts_speculated
    else:
        drafter_cache, target_cache = None, None

        list_tokens_id = eos_tokens_id if isinstance(eos_tokens_id, list) else [eos_tokens_id]
        stop_tokens = torch.tensor(list_tokens_id, dtype=torch.long, device=target.device).unsqueeze(1)
        
        drafts_accepted, drafts_speculated = .0, .0
        
        vocabulary_size = target.config.vocab_size    
            
        # prepare input tensor
        prompt_len = len(inputs)
        max_seq_length = target.config.max_position_embeddings if hasattr(target.config, 'max_position_embeddings') else (target.config.max_context_length if hasattr(target.config, 'max_context_length') else 1024)
        total_len = min(max_seq_length, prompt_len + max_gen_len)
        input_ids = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=target.device)
        input_ids[0, :prompt_len] = torch.tensor(inputs, dtype=torch.long, device=target.device)
        
        current_position = prompt_len
        
        if first_target:
            # run the target model before the speculative algorithm. Allows to prefill the kvcache and get a first token.
            Mp = target(
                input_ids=input_ids[..., :current_position],
                past_key_values=target_cache,
                use_cache=use_cache,
            )
            target_cache = Mp.past_key_values
            p_p = logits_processor(Mp.logits[..., -1, :])
            t = logits_processor.sample(p_p)
            input_ids[0, current_position] = t
            current_position += 1
            
            if torch.isin(t, stop_tokens):
                if debug:
                    printing.end_token_found(0)
                return input_ids[0, prompt_len:current_position].tolist(), 0
            
            if debug:
                printing.initial_step(t, tokenizer)
        
        while current_position < total_len:
            corrected_gamma = min(gamma, total_len - current_position - 1)
            q = torch.zeros((1, corrected_gamma, vocabulary_size), device=target.device)
            
            input_ids = input_ids.to(drafter.device)
            
            # generate gamma drafts
            for k in range(corrected_gamma):
                Mq = drafter(
                    input_ids=input_ids[..., :current_position + k],
                    past_key_values=drafter_cache,
                    use_cache=use_cache,
                )
                drafter_cache = Mq.past_key_values
                
                draft_logits = Mq.logits[..., -1, :]
                draft_probs = logits_processor(draft_logits)
                q[0, k] = draft_probs.to(target.device)
                xi = logits_processor.sample(draft_probs)
                input_ids[0, current_position + k] = xi
            drafts_speculated += corrected_gamma
            input_ids = input_ids.to(target.device)
            
            # run target model on drafts and get logits of the previous tokens plus one more token
            Mp = target(
                input_ids=input_ids[..., :current_position + corrected_gamma],
                past_key_values=target_cache,
                use_cache=use_cache,
            )
            target_cache = Mp.past_key_values
            draft_logits = Mp.logits[..., current_position - 1:current_position + corrected_gamma - 1, :] # [1, corrected_gamma, vocab_size]
            p = logits_processor(draft_logits) # [1, gamma, vocab_size]
            
            # compute the last accepted draft position (rejection sampling)
            r = torch.rand(corrected_gamma, device=target.device)
            fractions = p / q
            n = corrected_gamma #if all accept
            for i in range(corrected_gamma):
                if r[i] > fractions[0, i, input_ids[0, current_position + i]]:
                    n = i
                    break
            
            drafts_accepted += n
            
            # check if the end token is in the drafts
            stop_locations = torch.nonzero(torch.eq(input_ids[..., current_position:current_position + n], stop_tokens))
            if stop_locations.shape[0] > 0:
                stop_location = stop_locations[0, 1].item()
                if debug:
                    printing.end_token_found(stop_location)
                return input_ids[0, prompt_len:current_position + stop_location + 1].tolist(), drafts_accepted / drafts_speculated

            # adjust the distribution from Mp
            if n == corrected_gamma:
                p_p = Mp.logits[..., current_position + corrected_gamma - 1, :]
                p_p = logits_processor(p_p)
            else:
                # prune the cache
                if use_cache:
                    drafter_cache = prune_cache(drafter_cache, corrected_gamma - n)
                    target_cache = prune_cache(target_cache, corrected_gamma - n + 1)
                
                if not skip_sample_adjustment:
                    p_p = max_fn(p[..., n, :] - q[0, n, :])
                else:
                    p_p = p[..., n, :]
            x = logits_processor.sample(p_p)
            
            if debug:
                generated = input_ids.clone().detach()
                
            input_ids[0, current_position + n:current_position + corrected_gamma] = pad_token_id
            input_ids[0, current_position + n] = x
            
            if debug:
                printing.speculative_step(tokenizer, generated, input_ids, n, prompt_len, current_position, corrected_gamma)
                
            current_position += n + 1
            
            if torch.isin(x, stop_tokens):
                if debug:
                    printing.end_token_found(n)
                return input_ids[0, prompt_len:current_position].tolist(), drafts_accepted / drafts_speculated
    
    return input_ids[0, prompt_len:].tolist(), drafts_accepted / drafts_speculated